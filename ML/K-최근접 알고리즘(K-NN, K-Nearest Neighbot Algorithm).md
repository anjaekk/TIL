# K-최근접 알고리즘(K-NN, K-Nearest Neighbot Algorithm)

## 분류(Classification)과 회귀(Regression) 차이

### 분류

- 예측해야할 대상(class)가 정해져 있음

→ K-NN 분류: 새로운 데이터 x가 주어졌을 때, x에 가장 가까운 k개의 데이터중 가장 많은 값의 class로 판단

예) 이 데이터는 도미일 가능성이 높다.

### 회귀

- 연속성 중 어디에 해당하는가를 예측

→ K-NN 회귀: 새로운 데이터 x가 주어졌을 때, x에 가장 가까운 k개의 데이터 값을 평균내서 값을 예측

예) 이 데이터의 크기가 이정도이니 몸무게는 이정도 일 가능성이 높다.

K-최근접 알고리즘은 분류(Classification) 알고리즘으로 비슷한 특성을 가진 데이터는 비슷한 범주에 속하는 경향이 있다는 가정하에 사용된다. 

### 사용분야

- 이미지처리
- 글자 / 얼굴 인식
- 추천 알고리즘
- 의료 분야 등..

### 장점

- 단순하기 때문에 다른 알고리즘에 비해 구현이 쉽다
- 훈련데이터를 그대로 가지고 있어 특별한 훈련을 하지 않기 때문에 훈련 단계가 빠르게 수행된다.

### 단점

- 데이터 범위 밖의 새로운 데이터는 예측이 불가능함
- 모델을 생성하지 않기 때문에 클래스간 관례를 이해하는데 제한적
- 미리 변수와 클래스 간의 관계를 파악하여 이를 알고리즘의 적용해야한다.
- 데이터가 많아지면 분류 단계가 느리다.

### K값

- 홀수 지정 권장
- 과적합문제를 최소하기 위해 트레인 데이터 값의 성공률과 테스트 데이터 값의 성공률의 갭이 적게 차이나는 값을 설정해야한다.
- 테스트셋이 달라질 때마다 k값을 다르게 지정해야 한다.

### 과대적합(over fitting)

- 훈련 셋의 점수보다 테스트 셋의 점수가 지나치게 낮은 경우
- 훈련 셋에는 잘 맞지만 일반성이 떨어지는 경우
- 특정 훈련 셋에만 너무 훈련이 되어있게 되면 발생

### 과소적합(under fitting)

- 훈련 셋보다 테스트 셋의 점수가 높거나 두 점수가 모두 너무 낮은 경우
- 모델이 너무 단순해서 데이터의 내재된 구조를 학습하지 못하는 경우를 말한다.
- 해결을 위해 모델을 더 복잡하게 만들어야한다.
- K-NN에서는 K값을 줄여서 모델 값을 복잡하게 가능

# K-최근접 회귀(K-NN Regression)

## 회귀

우선 회귀란 여러 개의 독립변수와 한 개의 종속변수 간의 상관관계를 모델링하는 기법을 말한다. 머신러닝 에서 독립변수는 = feature에 해당하고 종속변수는 = 결정 값에 해당하게 된다. 여기서 파악해야하는 건 주어진 feature와 결정 값 데이터에서 학습을 통해 **최적의 회귀 계수를 찾아내는 것**이다.

## K-최근접 회귀

주변의 가장 가까운 K개의 샘플을 통해 값을 예측하는 방식

# 실습1-K-NN 분류

# < 데이터셋과 트레인셋 분리 전 >

## 데이터 정제

### ****Pyplot 라이브러리 설치****

```xml
python -m pip install -U pip
python -m pip install -U matplotlib
```

- matplotlib.pyplot은 MATLAB과 비슷하게 명령어 스타일로 동작하는 함수 모음
- 간편하게 그래프를 만들고 변화를 줄 수 있음

### 도미(bream)와 빙어(smelt) 데이터 추가

```xml
#도미 35마리 데이터(길이, 무게)
bream_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0, 31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0, 35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0] 
bream_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0, 500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0, 700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0]

#빙어 14마리 데이터(길이, 무게)
smelt_length = [9.8, 10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0] 
smelt_weight = [6.7, 7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
```

### 도미와 빙어 데이터 기반 그래프

```xml
from K_NN_data import * 
import matplotlib.pyplot as plt 

plt.scatter(bream_length, bream_weight) 
plt.scatter(smelt_length, smelt_weight) 
plt.xlabel('length') 
plt.ylabel('weight') 
plt.show()
```

- scatter: 산점도 그리는 함수

→ 물고기 길이에 따라 무게가 많이 나가서 선형적 그래프가 그려지게 된다.

### 도미와 빙어 데이터 합치기

```xml
length = bream_length + smelt_length 
weight = bream_weight + smelt_weight

# 2차원 리스트 생성
fish_data = [[l, w] for l, w in zip(length, weight)]

fish_target = [1] * 35 + [0] * 14
```

- 빙어는 0, 도미는 1로 하여 정답 데이터를 만든다.
- 길이와 무게 두가지의 feature를 이용한다.
    
    fish_data
    
    ```xml
    **[[25.4, 242.0], [26.3, 290.0], [26.5, 340.0], [29.0, 363.0], [29.0, 430.0], [29.7, 450.0], [29.7, 500.0], [30.0, 390.0], [30.0, 450.0], [30.7, 500.0], [31.0, 475.0], [31.0, 500.0], [31.5, 500.0], [32.0, 340.0], [32.0, 600.0], [32.0, 600.0], [33.0, 700.0], [33.0, 700.0], [33.5, 610.0], [33.5, 650.0], [34.0, 575.0], [34.0, 685.0], [34.5, 620.0], [35.0, 680.0], [35.0, 700.0], [35.0, 725.0], [35.0, 720.0], [36.0, 714.0], [36.0, 850.0], [37.0, 1000.0], [38.5, 920.0], [38.5, 955.0], [39.5, 925.0], [41.0, 975.0], [41.0, 950.0], [9.8, 6.7], [10.5, 7.5], [10.6, 7.0], [11.0, 9.7], [11.2, 9.8], [11.3, 8.7], [11.8, 10.0], [11.8, 9.9], [12.0, 9.8], [12.2, 12.2], [12.4, 13.4], [13.0, 12.2], [14.3, 19.7], [15.0, 19.9]]**
    ```
    
    fish_target
    
    ```xml
    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]
    ```
    

## 훈련

### 사이킷런(Scikit-learn) 설치

```xml
pip install scikit-learn
```

- 파이썬을 대표하는 머신러닝 오픈소스 라이브러리
- 무료(개인, 비지니스 모두)
- 사이킷런 페이지에 가면 무료로 이용할 수 있는 데이터도 있음

### 사이킷런(Scikit-learn)의 K-NN 분류 클래스 이용

```xml
from sklearn.neighbors import KNeighborsClassifier 
kn = KNeighborsClassifier()
```

- KNeighborsClassifier(): K-NN 분류 클래스
- K 기본값: 5
    - n_neighbors 매개변수로 변경 가능

### 도미 찾기 기준 학습

```xml
kn.fit(fish_data, fish_target) 
print(kn.score(fish_data, fish_target)) # 1.0
```

- fit 메서드를 통해 도미 fish_data를 이용해 target생선이 맞는지 학습한다.
- score 메소드를 이용하여 훈련 평가
    - 0~1 사이의 값 반환
    - 1은 모든 데이터를 정확히 맞춘 것

### K값 변경

```xml
kn = KNeighborsClassifier(n_neighbors=49) 
kn.fit(fish_data, fish_target) 
print(kn.score(fish_data, fish_target)) # 0.7142857142857143
```

- K를 49로 줬을 때(근처 49개의 데이터를 이용해 어떤 생선인지 확인) → 정확도: 35/49 = 0.7142857142857143
- 전체 데이터가 49개이니, k값이 중요하다는 걸 알 수 있다.

# < 트레인 셋과 테스트 셋 분리>

## 훈련셋(Train set)과 테스트 셋 분리

- 각 데이터 셋은 샘플이 고루 섞여 있어야 한다.
    - 샘플링 편향: 샘플링이 한쪽으로 치우쳐져 있는 경우
    

### 넘파이 설치

```xml
pip install numpy
```

- 데이터 섞기, 골고루 샘플 데이터 뽑아서 훈련셋과 테스트셋 만들기

### 리스트를 넘파이 array로 변경

```xml
import numpy as np 

input_arr = np.array(fish_data) 
target_arr = np.array(fish_target)
```

### 무작위 샘플 선택

```xml
np.random.seed(42) 
index = np.arange(49) # [0, 1, 2, ... , 48] 
np.random.shuffle(index) 

train_input = input_arr[index[:35]] 
train_target = target_arr[index[:35]]
```

- 섞은 후 35개의 샘플 선택
- 일정한 결과를 얻기위한 random seed 설정
